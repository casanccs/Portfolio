{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prerequisites\n\n1. Transpose\n2. Change of Basis\n3. Diagonlizability","metadata":{}},{"cell_type":"markdown","source":"# What is a Normal Matrix?","metadata":{}},{"cell_type":"markdown","source":"A matrix, $A$, is normal if:\n\n$$AA^* = A^*A$$\n\nWhere $*$ denotes the conjugate transpose. But since we are dealing with only real numbers:\n\n$$AA^T = A^TA$$","metadata":{}},{"cell_type":"markdown","source":"A consequence of this, is that we find that a matrix is normal if and only if it is unitaryily diagonalizable. In other words, if you can diagonalize a matrix with it's change of basis matrix being a unitary matrix. The proof for this is complicated, but it's shown by using the Schur decomposition. But what is a unitary matrix?","metadata":{}},{"cell_type":"markdown","source":"# Orthonormal vectors","metadata":{}},{"cell_type":"markdown","source":"We know that orthogonal vectors are vectors that are perpendicular to each other. But how can we tell when two vectors are perpendicular? We use something called the **Inner Product** which is a general case of dot product which only applies to Euclidean spaces. The definition of the inner product is weird because it requires two vectors, and is a scalar, just like the dot product. So the inner product is a scalar value that is associated with each pair of vectors. Inner product between vectors, $v$ and $u$, is $\\langle v,u \\rangle $. The true definition is:\n\n$$\\langle \\cdot , \\cdot \\rangle : \\mathbf{V} \\times \\mathbf{V} \\rightarrow \\mathbf{F}$$\n\nAs a guy learning about inner products, I realized that inner products is not something that is easily defined. Instead, you need to find the things that **SATISFY** being an inner product. Just like how there are requirements from spaces to be a proper vector space, there are properties that need to be satisfied for something to be called an inner prdouct. In our case, the real *n*-space with dot product is an inner product, because it fulfills the properties that you can find here: (https://en.wikipedia.org/wiki/Inner_product_space). Here we will focus on the orthogonal property, which is:\n\n$$\\langle v,u \\rangle = 0$$\n\nWhich means that if you take the inner product of $v$ and $u$, and it gives you $0$, then $v$ and $u$ are orthogonal. How do we know? Since we are using the Euclidean vector space dot product as our inner product, we define the inner product as:\n\n$$\\langle v,u \\rangle = \\sum_{i=0}^n{v_iu_i}$$\n\n**Proof Goes Here**\n\nNow we know what it means for vectors to be orthogonal. But what about **Orthonormal** vectors? Orthonormal vectors are just two vectors that are orthogonal, and are both length $1$. An orthonormal set is a set of vectors that are all orthonormal to each other. So by definition, vector space, $\\mathbf{V}$, is orthonormal if and only if:\n\n$$\\forall i,j: \\langle v_i, v_j \\rangle = \\delta_{ij}$$\n\nWhere $\\{u_1,u_2,...u_n\\}$ is in the inner-product space, $\\mathcal{V}$. And where:\n\n$$\\delta_{ij} = \\left\\{ \\begin{array}{ccc} 1 & \\mbox{if} & i = j \\\\ 0 & \\mbox{if} & i \\neq j \\end{array}\\right.$$\n\nThe standard basis is a trivial example of orthonormal vectors.","metadata":{}},{"cell_type":"markdown","source":"# What is an Orthogonal/Unitary Matrix?","metadata":{}},{"cell_type":"markdown","source":"A matrix, A , is orthogonal/unitary if:\n\n$$AA^{T} = A^{T}A = I$$\n\nIn other words, a matrix is orthogonal if it's transpose is it's inverse. However the other requirement, is that it's row and columns must be orthogonal unit vectors, AKA, **Orthonormal vectors**. Because of the fact that the matrix is made of unit vectors that are orthogonal to each other, then the determinant of this orthogonal matrix is always either $1$ or $-1$, meaning that it is always a unitary transformation. These transformations consist of rotations, reflections, and rotoreflections.","metadata":{}}]}