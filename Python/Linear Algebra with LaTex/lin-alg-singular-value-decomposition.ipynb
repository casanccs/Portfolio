{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* What you should know before diving into SVD:\n    1. Basis vectors and its variants\n    2. Unitary/orthogonal matrices\n    3. What it means to be \"eigen\"\n    4. Normal Matrix\n    6. Diagonalizability\n    5. Orthonormal\n    6. Eigendecomposition","metadata":{}},{"cell_type":"markdown","source":"# Why do we need Singular Value Decomposition? (SVD)\n\n* SVD is very useful for Dimensonality Reduction and solving certain optimization problems with alot of data.\n    1. Solve $Ax = b$ (Non-square A)\n    2. Regression\n    3. Basis Principal Component Analysis\n    4. Take large number of features, and give us the core features that is smaller than the original number.\n* SVD is a data driven generalization of the Fourier Transform. (FFT)\n* The SVD gives us a coordinate system/transformation depending on our data.","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"}},{"cell_type":"markdown","source":"# What is the Singular Value Decomposition\n\nLets say that we have some Data Matrix, $\\mathbf{X}$, that has $m$ data examples, and $n$ dimensions per example. We will interpret this as an $nxm$ matrix:\n$$\\mathbf{X} = \\left[ \\begin{array}{cccc} | & | & ... & |\\\\ X_1 & X_2 & ... & X_m \\\\ | & | & ... & | \\end{array}\\right]$$\nWhere each $X_i$ $\\epsilon$ $\\mathbb{R}^n$. $$$$\nWe will then \"decompose\" this $\\mathbf{X}$.\n$$\\mathbf{X} = \\left[ \\begin{array}{cccc} | & | & ... & |\\\\ X_1 & X_2 & ... & X_m \\\\ | & | & ... & | \\end{array}\\right] = \\mathbf{U}\\Sigma\\mathbf{V}^T$$\nWhere:\n* $\\mathbf{U}$ (left singular vectors) and $\\mathbf{V}$ (right singular vectors) are unitary/orthogonal matrices.\n* $\\Sigma$ (singular values) is a rectangular diagonal matrix.\n$$\\mathbf{X} = \\left[ \\begin{array}{cccc} | & | & ... & |\\\\ X_1 & X_2 & ... & X_m \\\\ | & | & ... & | \\end{array}\\right] = \\mathbf{U}\\Sigma\\mathbf{V}^T = \n\\left[\\begin{array}{ccc} | & ... & | \\\\ u_1 & ... & u_n \\\\ | & ... & |\\end{array}\\right]\\left[\\begin{array}{ccc} \\sigma_1 & ... & 0 \\\\  ...& ... & ... \\\\ 0 & ... & \\sigma_m \\\\ 0 & ... & 0\\end{array}\\right]\n\\left[\\begin{array}{ccc} | & ... & | \\\\ v_1 & ... & v_m \\\\ | & ... & |\\end{array}\\right]^T$$\n* Columns of $\\mathbf{U}$ is of the same shape of the columns in $\\mathbf{X}$.\n* The columns of $\\mathbf{U}$ are \"eigen\" columns, and are hierarchically arranged so $u_1$ is more important to $u_2$, etc.\n* $\\mathbf{U}^T\\mathbf{U} = \\mathbf{U}\\mathbf{U}^T = \\mathbf{I}$, same thing for $\\mathbf{V}$.\n* $\\Sigma$ is non-negative and is decreasing as you go to the right.","metadata":{}},{"cell_type":"markdown","source":"### Importance of $\\mathbf{U}$, $\\mathbf{V}$, and $\\Sigma$\n   1. $\\mathbf{U}$ is used to be the basis of $\\mathbf{X}$.\n   2. $\\Sigma$'s purpose is to show how important each $u_i$ is.\n   3. $\\mathbf{V}^T$ will make the whole thing equal to $\\mathbf{X}$.","metadata":{}},{"cell_type":"markdown","source":"# Interpretations of SVD","metadata":{}},{"cell_type":"markdown","source":"### Linear Transformations\n\nIf $\\mathbf{X}$ is a real square matrix, then that means both $\\mathbf{U}$ and $\\mathbf{V}$ can also be square and thus be orthonormal. We can then interpret $\\mathbf{X}$ as a linear transformation from the three matrices that we get from decomposing $\\mathbf{X}$. So if given some matrix, $x$, we can apply a linear transformation, the SVD. This means we can achieve $\\mathbf{X}$ from applying 3 different actions to $x$.\n\n$$\\mathbf{X}x = (\\mathbf{U}\\Sigma\\mathbf{V}^T)x$$\n\n* We interpret this as:\n    1. $\\mathbf{V}^T$ will be the rotation or reflection of the space.\n    2. $\\Sigma$ will scale the coordinates individually.\n    3. $\\mathbf{U}$ will be the final rotation or reflection.\n    \nIf $\\mathbf{X}$ is a real rectangular matrix, $nxm$, then we can see this as a linear transformation from $\\mathbb{R}^m$ to $\\mathbb{R}^n$.\n\n* A key note is that we can view the singular values as:\n    1. Magnitudes of the semiaxes of an ellipsoid in terms of n-dimensions in an n-d space, assuming $\\mathbf{X}$ is an $nxn$ matrix.\n    2. Magnitudes of the semiaxes of an ellipsoid in terms of m-dimensions in an n-d space, assuming $\\mathbf{X}$ is an $nxm$ matrix.\n* While the singular vectors encode the directions of the semiaxes, and form a set of orthonormal vectors which makes a basis for $\\mathbf{X}$.\n    - Take a basis vector, $\\mathbf{V_i}$ from $\\mathbf{V}^T$, and apply $\\mathbf{X}$ to it. This will map $\\mathbf{V_i}$ to the stretched unit vector, $\\sigma_i\\mathbf{u_i}$. This rule can be applied to $\\mathbf{U}$, $\\mathbf{U}^T$, $\\mathbf{V}$, and $\\mathbf{V}^T$ since they are orthonormal, making these orthonormal bases.","metadata":{}},{"cell_type":"markdown","source":"**IN THE WORKS!!** Here is a visualization of a linear transformation from an SVD:","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}