{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Goals with this Kernel\n* Learn what linear models are, and their purpose.\n* Utilize them and find out each pro and con.\n* Practice basic Data Visualization","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"}},{"cell_type":"markdown","source":"# What is a Linear Model?","metadata":{}},{"cell_type":"markdown","source":"* In Algebra, we are introduced with the most basic linear equation: $y = mx + b$. Here, there is a direct correlation between $y$ and $x$, where we see that when $x$ changes, $y$ will scale to $x$ considering the coefficient that scales $x$ and it's \"bias\"/intercept, $b$.\n* In higher dimensions, we see that we discover a new terminology called the **Linear Combination**. This is very similar to the linear equation above, except applies to higher dimensions.\n* Linear Combination $\\rightarrow$ $y = \\alpha_1x_1 + \\alpha_2x_2 + $ ... $\\alpha_nx_n$ , where $\\alpha_i$ $\\epsilon$ $\\mathbb{R}$, $n$ $=$ (# of dimensions), and $x_i$ is the range of possible values.\n* In regression, since we are predicting a target value, Linear Models are Models where we expect the target value to be a form of Linear Combination of features:\n$$\\hat{y}(w,x) = w_0 + w_1x_1 + ... + w_px_p$$\nWhere $w_j$ is the weight coefficient, $x_j$ is the feature value, and $p$ is the number of features. Note that here: $x_0 = 1$.\nKeep in mind that scikit-learn makes the $w = (w_1,w_2,...,w_p)$ labeled as [coef_] and $w_0$ as [intercept_] , which is known as the bias.","metadata":{}},{"cell_type":"markdown","source":"## - Example of a problem:\nLet's say that we have a dataframe with only 1 feature. This yields:\n$$\\hat{y}(w,x) = w_0 + w_1x_1$$\nNow let's say that: $$w_0 = 2, w_1 = 0.5$$\nThis gives us the line/model: $$\\hat{y} = (0.5)x_1 + 2 $$\nWhich is drawn as:","metadata":{}},{"cell_type":"code","source":"w0 = 2. ; w1 = 0.5\nweights = np.array([w0,w1])\nx1 = np.arange(3)\nyhat = w0 + w1*x1\nplt.plot(x1,yhat)\nplt.ylabel(\"Predicted value (yhat)\")\nplt.xlabel(\"x1\")\nplt.ylim((0,4))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's say that we have a training value whose feature value, $x_1 = 0.75$, and whose true value is $y = 3.0$ at $x_1 = 0.75$.$$$$\nAccording to our model/estimator, we are predicting, $\\hat{y} = 0.5*0.75 + 2 = 2.375$, so our model is off by $3$ $-$ $2.375$ $ = $ $0.625$.","metadata":{}},{"cell_type":"code","source":"example_x1 = 0.75 ; example_Value = 3.\nexample = pd.DataFrame({\"x1\": [0.75], \"Value\" : [3]})\nexample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is our single example in a DataFrame which we will now plot.","metadata":{}},{"cell_type":"code","source":"plt.plot(x1,yhat)\nplt.scatter(example.loc[:,\"x1\"],example.loc[:,\"Value\"])\nplt.ylabel(\"y\")\nplt.xlabel(\"x1\")\nplt.ylim((0,4))\nplt.show()\n#I plan to make a drawing from the point to the line indicating that this is the distance.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there is a bit of distance from our model and the true value. This is error. From our human standpoint, all we would do to fix this, would be either to increase the bias/$w_0$ to $2.625$:\n$$\\hat{y} = 0.5*0.75 + 2.625 = 3$$\nOr increase the slope/$w_1$ to $\\frac{4}{3}$:\n$$\\hat{y} = \\frac4{3}*0.75 + 2 = 3$$\nEach individual Linear Model that is on here will treat error differently, and will adjust it's line accordingly to the error.","metadata":{}},{"cell_type":"markdown","source":"# Linear Regression: Ordinary Least Squares","metadata":{}},{"cell_type":"markdown","source":"First we must know what the **Sum of Squares** is.\n* Sum of Squares can be obtained by getting the distance of every data point from the mean of the data, squaring the distances, then summing those squared distance:\n  $$SS = \\sum_i{(y_i - \\overline{y})^2}$$\n  Where $y_i$ represents the $ith$ data in the set, and $\\overline{y}$ represents the mean of the data.\n* Sum of Squares is also known as variation.\n* Also note that the Sum of Squares has nothing to do with a Model/Line.","metadata":{}},{"cell_type":"code","source":"#Example of Sum of Squares\ndata = pd.DataFrame({\"feature\":[0,3,5,7,10],\"Value\": [0,1,4,3,8]})\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_mean = data.Value.mean() #Here we have the mean, (0+1+4+3+8)/5 = 3.2\ndata_distance = data.Value.values - data_mean #Here is the (y_i - mean)\ndata_dist_squared = np.power(data_distance,2) #Here is the (y_i - mean)^2\nSum_of_Squares = np.sum(data_dist_squared) #Here is the addition of all squared distances\nprint(\"Mean of data is: \", data_mean)\nprint(\"Distance of data from mean is: \", data_distance)\nprint(\"Squared distance of data from mean is: \", data_dist_squared)\nprint(\"Sum of Squares = \", np.around(Sum_of_Squares,2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice that none of that had anything to do with our model. The distance we are calculating in the Sum of Squares is this distance:","metadata":{}},{"cell_type":"code","source":"x1 = data.feature\ndata_mean = np.full(x1.size,data_mean)\nplt.plot(x1, data_mean)\nplt.scatter(data.feature, data.Value)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Why sum of squares? Why do we square the distance? Why not just take the absolute value to find the distance?\n    1. Emphasize datapoints that are further\n    2. The algebra is easier","metadata":{}},{"cell_type":"markdown","source":"Next, we need to know what the **Residual Sum of Squares** is.\n* The difference between the Normal vs. Residual SS, is that Residual instead takes a model and quantifies how much that model is off by. Or in words that are found in documentations and other sources, \"how much of the dependent variableâ€™s variation your model did not explain.\"\n* Residual Sum of Squares:\n$$\\sum_i{(y_i - \\hat{y}_i)^2}$$","metadata":{}},{"cell_type":"code","source":"yhat = w0 + w1*x1\nprint(data)\nplt.plot(x1,yhat)\nplt.scatter(data.feature ,data.Value)\nplt.xlabel(\"feature\")\nplt.ylabel(\"Value\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Finding the Residual Sum of Squares of our model and data\nmodel_distance = (data.Value - yhat).values #(y - predicted_y)\nmodel_distance_squared = np.power(model_distance,2) #(y - predicted_y)^2\nResidual_Sum_of_Squares = np.sum(model_distance_squared)\nprint(\"Distance of data from prediction: \", model_distance)\nprint(\"Squared Distance of data from prediction: \", model_distance_squared)\nprint(\"Residual Sum of Squares = \", Residual_Sum_of_Squares)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now know the Residual Sum of Squares of our model. The goal to have the best Linear Model, is to find weight coefficients to minimize this Residual Sum of Squares. How will we do this?","metadata":{}},{"cell_type":"markdown","source":"Linear Regression/Ordinary Least Squares.","metadata":{}},{"cell_type":"markdown","source":"Using the [LinearRegression] Linear Model from scikit-learn, [LinearRegression] takes a linear model and tries to minimize the residual sum of squares between the true values (observed values) and predicted values. In other words, take all of the distances that you get from predicted value to true value, square them, then add up those square distances. Then find out if there is a way to decrease that distance by adjusting the weights until you find the minimum distances.","metadata":{}},{"cell_type":"markdown","source":"According to Scikit-Learn, the [LinearRegression] estimator solves the mathematical problem:\n$$\\min_w{\\Vert{Xw} - y\\Vert^2_2}$$\nHere, $X = (x_1,x_2...x_p), w = (w_1,...w_p), y = (values)$. But don't forget that $Xw$ is actually the prediction, $\\hat{y}$.\n* The solution is found by using the **Singular Value Decomposition** of X.","metadata":{}},{"cell_type":"markdown","source":"## Linear Regression Caveats","metadata":{}},{"cell_type":"markdown","source":"1. Linear Regression depends on how independent the features are from one another. This means that the more correlation they have, the \"design matrix\"/dataframe becomes close to singular: It slowly has a determinant that is not 0, therefore not having an inverse.\n2. The OLS estimator is accurate when the regressors (data) are ","metadata":{}}]}